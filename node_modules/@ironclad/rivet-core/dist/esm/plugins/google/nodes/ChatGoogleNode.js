import {} from '../../../index.js';
import { streamChatCompletions, streamGenerativeAi, generativeAiGoogleModels, generativeAiOptions, } from '../google.js';
import { nanoid } from 'nanoid/non-secure';
import { dedent } from 'ts-dedent';
import retry from 'p-retry';
import { match } from 'ts-pattern';
import { coerceType, coerceTypeOptional } from '../../../utils/coerceType.js';
import { addWarning } from '../../../utils/outputs.js';
import { getError } from '../../../utils/errors.js';
import { uint8ArrayToBase64 } from '../../../utils/base64.js';
import { pluginNodeDefinition } from '../../../model/NodeDefinition.js';
import { getScalarTypeOf, isArrayDataValue } from '../../../model/DataValue.js';
import { getInputOrData } from '../../../utils/inputs.js';
// Temporary
const cache = new Map();
export const ChatGoogleNodeImpl = {
    create() {
        const chartNode = {
            type: 'chatGoogle',
            title: 'Chat (Google)',
            id: nanoid(),
            visualData: {
                x: 0,
                y: 0,
                width: 275,
            },
            data: {
                model: 'gemini-2.0-flash-001',
                useModelInput: false,
                temperature: 0.5,
                useTemperatureInput: false,
                top_p: 1,
                useTopPInput: false,
                top_k: undefined,
                useTopKInput: false,
                useTopP: false,
                useUseTopPInput: false,
                maxTokens: 1024,
                useMaxTokensInput: false,
                cache: false,
                useAsGraphPartialOutput: true,
            },
        };
        return chartNode;
    },
    getInputDefinitions(data) {
        const inputs = [];
        inputs.push({
            id: 'systemPrompt',
            title: 'System Prompt',
            dataType: 'string',
            required: false,
            description: 'An optional system prompt for the model to use.',
        });
        if (data.useModelInput) {
            inputs.push({
                id: 'model',
                title: 'Model',
                dataType: 'string',
                required: false,
            });
        }
        if (data.useTemperatureInput) {
            inputs.push({
                dataType: 'number',
                id: 'temperature',
                title: 'Temperature',
            });
        }
        if (data.useTopPInput) {
            inputs.push({
                dataType: 'number',
                id: 'top_p',
                title: 'Top P',
            });
        }
        if (data.useUseTopPInput) {
            inputs.push({
                dataType: 'boolean',
                id: 'useTopP',
                title: 'Use Top P',
            });
        }
        if (data.useMaxTokensInput) {
            inputs.push({
                dataType: 'number',
                id: 'maxTokens',
                title: 'Max Tokens',
            });
        }
        inputs.push({
            dataType: ['chat-message', 'chat-message[]'],
            id: 'prompt',
            title: 'Prompt',
        });
        return inputs;
    },
    getOutputDefinitions(data) {
        const outputs = [];
        outputs.push({
            dataType: 'string',
            id: 'response',
            title: 'Response',
        });
        outputs.push({
            dataType: 'chat-message[]',
            id: 'in-messages',
            title: 'Messages Sent',
            description: 'All messages sent to the model.',
        });
        outputs.push({
            dataType: 'chat-message[]',
            id: 'all-messages',
            title: 'All Messages',
            description: 'All messages, with the response appended.',
        });
        return outputs;
    },
    getBody(data) {
        return dedent `
      ${generativeAiGoogleModels[data.model]?.displayName ?? `Google (${data.model})`}
      ${data.useTopP
            ? `Top P: ${data.useTopPInput ? '(Using Input)' : data.top_p}`
            : `Temperature: ${data.useTemperatureInput ? '(Using Input)' : data.temperature}`}
      Max Tokens: ${data.maxTokens}
    `;
    },
    getEditors() {
        return [
            {
                type: 'dropdown',
                label: 'Model',
                dataKey: 'model',
                useInputToggleDataKey: 'useModelInput',
                options: generativeAiOptions,
            },
            {
                type: 'number',
                label: 'Temperature',
                dataKey: 'temperature',
                useInputToggleDataKey: 'useTemperatureInput',
                min: 0,
                max: 2,
                step: 0.1,
            },
            {
                type: 'number',
                label: 'Top P',
                dataKey: 'top_p',
                useInputToggleDataKey: 'useTopPInput',
                min: 0,
                max: 1,
                step: 0.1,
            },
            {
                type: 'toggle',
                label: 'Use Top P',
                dataKey: 'useTopP',
                useInputToggleDataKey: 'useUseTopPInput',
            },
            {
                type: 'number',
                label: 'Max Tokens',
                dataKey: 'maxTokens',
                useInputToggleDataKey: 'useMaxTokensInput',
                min: 0,
                max: Number.MAX_SAFE_INTEGER,
                step: 1,
            },
            {
                type: 'toggle',
                label: 'Cache (same inputs, same outputs)',
                dataKey: 'cache',
            },
            {
                type: 'toggle',
                label: 'Use for subgraph partial output',
                dataKey: 'useAsGraphPartialOutput',
            },
        ];
    },
    getUIData() {
        return {
            infoBoxBody: dedent `
        Makes a call to an Google chat model. The settings contains many options for tweaking the model's behavior.
      `,
            infoBoxTitle: 'Chat (Google) Node',
            contextMenuTitle: 'Chat (Google)',
            group: ['AI'],
        };
    },
    async process(data, inputs, context) {
        const output = {};
        const systemPrompt = coerceTypeOptional(inputs['systemPrompt'], 'string');
        const rawModel = getInputOrData(data, inputs, 'model');
        const model = rawModel;
        const temperature = getInputOrData(data, inputs, 'temperature', 'number');
        const topP = getInputOrData(data, inputs, 'top_p', 'number');
        const useTopP = getInputOrData(data, inputs, 'useTopP', 'boolean');
        const { messages } = getChatGoogleNodeMessages(inputs);
        const prompt = await Promise.all(messages.map(async (message) => {
            return {
                role: message.type === 'user' ? 'user' : 'assistant',
                parts: await Promise.all([message.message].flat().map(async (part) => {
                    if (typeof part === 'string') {
                        return { text: part };
                    }
                    else if (part.type === 'image') {
                        return {
                            inline_data: {
                                mime_type: part.mediaType,
                                data: (await uint8ArrayToBase64(part.data)),
                            },
                        };
                    }
                    else {
                        throw new Error(`Google Vertex AI does not support message parts of type ${part.type}`);
                    }
                })),
            };
        }));
        let { maxTokens } = data;
        const tokenizerInfo = {
            node: context.node,
            model,
            endpoint: undefined,
        };
        // TODO Better token counting for Google models.
        const tokenCount = await context.tokenizer.getTokenCountForMessages(messages, undefined, tokenizerInfo);
        if (generativeAiGoogleModels[model] && tokenCount >= generativeAiGoogleModels[model].maxTokens) {
            throw new Error(`The model ${model} can only handle ${generativeAiGoogleModels[model].maxTokens} tokens, but ${tokenCount} were provided in the prompts alone.`);
        }
        if (generativeAiGoogleModels[model] && tokenCount + maxTokens > generativeAiGoogleModels[model].maxTokens) {
            const message = `The model can only handle a maximum of ${generativeAiGoogleModels[model].maxTokens} tokens, but the prompts and max tokens together exceed this limit. The max tokens has been reduced to ${generativeAiGoogleModels[model].maxTokens - tokenCount}.`;
            addWarning(output, message);
            maxTokens = Math.floor((generativeAiGoogleModels[model].maxTokens - tokenCount) * 0.95); // reduce max tokens by 5% to be safe, calculation is a little wrong.
        }
        const project = context.getPluginConfig('googleProjectId');
        const location = context.getPluginConfig('googleRegion');
        const applicationCredentials = context.getPluginConfig('googleApplicationCredentials');
        const apiKey = context.getPluginConfig('googleApiKey');
        if (!apiKey) {
            if (project == null) {
                throw new Error('Google Project ID or Google API Key is not defined.');
            }
            if (location == null) {
                throw new Error('Google Region or Google API Key is not defined.');
            }
            if (applicationCredentials == null) {
                throw new Error('Google Application Credentials or Google API Key is not defined.');
            }
        }
        try {
            return await retry(async () => {
                const options = {
                    prompt,
                    model,
                    temperature: useTopP ? undefined : temperature,
                    topP: useTopP ? topP : undefined,
                    maxOutputTokens: maxTokens,
                    systemPrompt,
                    topK: undefined,
                };
                const cacheKey = JSON.stringify(options);
                if (data.cache) {
                    const cached = cache.get(cacheKey);
                    if (cached) {
                        return cached;
                    }
                }
                const startTime = Date.now();
                let chunks;
                if (apiKey) {
                    chunks = streamGenerativeAi({
                        signal: context.signal,
                        model,
                        prompt,
                        maxOutputTokens: maxTokens,
                        temperature: useTopP ? undefined : temperature,
                        topP: useTopP ? topP : undefined,
                        topK: undefined,
                        apiKey,
                        systemPrompt,
                    });
                }
                else {
                    chunks = streamChatCompletions({
                        signal: context.signal,
                        model: model,
                        prompt,
                        max_output_tokens: maxTokens,
                        temperature: useTopP ? undefined : temperature,
                        top_p: useTopP ? topP : undefined,
                        top_k: undefined,
                        project: project,
                        location: location,
                        applicationCredentials: applicationCredentials,
                    });
                }
                const responseParts = [];
                for await (const chunk of chunks) {
                    if (!chunk.completion) {
                        // Could be error for some reason 🤷‍♂️ but ignoring has worked for me so far.
                        continue;
                    }
                    responseParts.push(chunk.completion);
                    output['response'] = {
                        type: 'string',
                        value: responseParts.join('').trim(),
                    };
                    context.onPartialOutputs?.(output);
                }
                const endTime = Date.now();
                output['all-messages'] = {
                    type: 'chat-message[]',
                    value: [
                        ...messages,
                        {
                            type: 'assistant',
                            message: responseParts.join('').trim() ?? '',
                            function_call: undefined,
                            function_calls: undefined,
                        },
                    ],
                };
                output['in-messages'] = {
                    type: 'chat-message[]',
                    value: messages,
                };
                if (responseParts.length === 0) {
                    throw new Error('No response from Google');
                }
                output['requestTokens'] = { type: 'number', value: tokenCount };
                const responseTokenCount = await context.tokenizer.getTokenCountForString(responseParts.join(''), tokenizerInfo);
                output['responseTokens'] = { type: 'number', value: responseTokenCount };
                // TODO
                // const cost =
                //   getCostForPrompt(completionMessages, model) + getCostForTokens(responseTokenCount, 'completion', model);
                // output['cost' as PortId] = { type: 'number', value: cost };
                const duration = endTime - startTime;
                output['duration'] = { type: 'number', value: duration };
                Object.freeze(output);
                cache.set(cacheKey, output);
                return output;
            }, {
                retries: 10,
                maxRetryTime: 1000 * 60 * 5,
                factor: 2.5,
                minTimeout: 500,
                maxTimeout: 5000,
                randomize: true,
                signal: context.signal,
                onFailedAttempt(err) {
                    context.trace(`ChatGoogleNode failed, retrying: ${err.toString()}`);
                    if (context.signal.aborted) {
                        throw new Error('Aborted');
                    }
                },
            });
        }
        catch (error) {
            context.trace(getError(error).stack ?? 'Missing stack');
            throw new Error(`Error processing ChatGoogleNode: ${error.message}`);
        }
    },
};
export const chatGoogleNode = pluginNodeDefinition(ChatGoogleNodeImpl, 'Chat');
export function getChatGoogleNodeMessages(inputs) {
    const prompt = inputs['prompt'];
    if (!prompt) {
        throw new Error('Prompt is required');
    }
    const messages = match(prompt)
        .with({ type: 'chat-message' }, (p) => [p.value])
        .with({ type: 'chat-message[]' }, (p) => p.value)
        .with({ type: 'string' }, (p) => [{ type: 'user', message: p.value }])
        .with({ type: 'string[]' }, (p) => p.value.map((v) => ({ type: 'user', message: v })))
        .otherwise((p) => {
        if (isArrayDataValue(p)) {
            const stringValues = p.value.map((v) => coerceType({
                type: getScalarTypeOf(p.type),
                value: v,
            }, 'string'));
            return stringValues.filter((v) => v != null).map((v) => ({ type: 'user', message: v }));
        }
        const coercedMessage = coerceType(p, 'chat-message');
        if (coercedMessage != null) {
            return [coercedMessage];
        }
        const coercedString = coerceType(p, 'string');
        return coercedString != null ? [{ type: 'user', message: coerceType(p, 'string') }] : [];
    });
    return { messages };
}
