import {} from '../NodeBase.js';
import { nanoid } from 'nanoid/non-secure';
import { NodeImpl } from '../NodeImpl.js';
import {} from '../ProcessContext.js';
import {} from '../../index.js';
import { dedent } from 'ts-dedent';
import { nodeDefinition } from '../NodeDefinition.js';
import { ChatNodeBase } from './ChatNodeBase.js';
export class ChatNodeImpl extends NodeImpl {
    static create() {
        const chartNode = {
            type: 'chat',
            title: 'Chat',
            id: nanoid(),
            visualData: {
                x: 0,
                y: 0,
                width: 200,
            },
            data: ChatNodeBase.defaultData(),
        };
        return chartNode;
    }
    getInputDefinitions() {
        return ChatNodeBase.getInputDefinitions(this.data);
    }
    getOutputDefinitions() {
        return ChatNodeBase.getOutputDefinitions(this.data);
    }
    static getUIData() {
        return {
            infoBoxBody: dedent `
        Makes a call to an LLM chat model. Supports GPT and any OpenAI-compatible API. The settings contains many options for tweaking the model's behavior.

        The \`System Prompt\` input specifies a system prompt as the first message to the model. This is useful for providing context to the model.

        The \`Prompt\` input takes one or more strings or chat-messages (from a Prompt node) to send to the model.
      `,
            contextMenuTitle: 'Chat',
            infoBoxTitle: 'Chat Node',
            group: ['Common', 'AI'],
        };
    }
    getEditors() {
        return ChatNodeBase.getEditors();
    }
    getBody() {
        return ChatNodeBase.getBody(this.data);
    }
    async process(inputs, context) {
        return ChatNodeBase.process(this.data, this.chartNode, inputs, context);
    }
}
export const chatNode = nodeDefinition(ChatNodeImpl, 'Chat');
