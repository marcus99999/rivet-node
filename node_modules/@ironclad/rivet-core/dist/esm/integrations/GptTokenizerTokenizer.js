import {} from '../index.js';
import Emittery from 'emittery';
import { getError } from '../utils/errors.js';
import { chatMessageToOpenAIChatCompletionMessage } from '../utils/chatMessageToOpenAIChatCompletionMessage.js';
export class GptTokenizerTokenizer {
    emitter = new Emittery();
    on(event, listener) {
        this.emitter.on(event, listener);
    }
    async getTokenCountForString(input, _info) {
        const { encode } = await import('gpt-tokenizer');
        return encode(input).length;
    }
    async getTokenCountForMessages(messages, functions, _info) {
        try {
            const openaiMessages = await Promise.all(messages.map((message) => chatMessageToOpenAIChatCompletionMessage(message)));
            const validMessages = openaiMessages
                .filter((message) => message.role !== 'tool')
                .map((message) => {
                if (Array.isArray(message.content)) {
                    const textContent = message.content
                        .filter((c) => c.type === 'text')
                        .map((c) => c.text)
                        .join('');
                    return { ...message, content: textContent };
                }
                return message;
            });
            const { encode, encodeChat } = await import('gpt-tokenizer');
            const encodedChat = encodeChat(validMessages, 'gpt-3.5-turbo');
            const encodedFunctions = functions && functions.length > 0 ? encode(this.convertGptFunctionsToPromptString(functions)) : [];
            return encodedChat.length + encodedFunctions.length;
        }
        catch (err) {
            // eslint-disable-next-line @typescript-eslint/no-floating-promises
            this.emitter.emit('error', getError(err));
            return 0;
        }
    }
    /**
     * Converts GPT Functions to approximate TypeScript-style string.
     * Per thread: https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/24
     * We should consider using a different library, eg. https://github.com/hmarr/openai-chat-tokens
     * @param functions
     */
    convertGptFunctionsToPromptString(functions) {
        return `
# Tools

## functions

namespace functions {
${functions
            .map((fn) => `
// ${fn.description}
type ${fn.name} = (_: {
${Object.entries(fn.parameters?.properties ?? {})
            .map(([parameterName, value]) => `// ${value?.description}\n${parameterName}?: ${value?.type}`)
            .join('\n')}
})
`)
            .join('')}
} // namespace functions
`;
    }
}
